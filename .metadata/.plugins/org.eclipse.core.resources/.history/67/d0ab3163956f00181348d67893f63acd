import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.SparkContext._
import org.apache.spark.streaming.twitter._
import org.apache.spark.SparkConf
import org.apache.spark.streaming.twitter.TwitterUtils

object TwitterStreaming {
  def main(args:Array[String]){

 consumerKey="MPynGjt7VuJhqj3EkKZvuqUDS"    
consumerSecret="ln6ohm9kX9IJUviFuE38QOKXAj5XQMENpd1WD3fKIFOJm2ZojZ"    
accessToken="826011494445936640VZIqiX3DMldmc4HzF4OX7hDyikEG1il"    
accessTokenSecret="nAFuElZN8RA90x3sNUHLHkMrcwKBkQ1JOpF1QZHgXJeIX"

 val filters=Array("coffee","hadoop","spark","kafka","xbox","ps4","nintendo")
 System.setProperty("twitter4j.oauth.consumerKey", consumerKey)
 System.setProperty("twitter4j.oauth.consumerSecret",consumerSecret)
 System.setProperty("twitter4j.oauth.accessToken", accessToken)
 System.setProperty("twitter4j.oauth.accessTokenSecret",accessTokenSecret)
 val sparkConf = new SparkConf().setAppName("TwitterPopularTags").setMaster("local[2]")
 val ssc = new StreamingContext(sparkConf, Seconds(10))
 val stream = TwitterUtils.createStream(ssc, None, filters)
 val hashTags = stream.flatMap(status => status.getText.split(" ").filter(_.startsWith("#")))
 
 val topCounts60 = hashTags.map((_, 1)).reduceByKeyAndWindow(_ + _,Seconds(60))
 .map{case (topic, count) => (count, topic)}
 .transform(_.sortByKey(false))
 val topCounts10 = hashTags.map((_, 1)).reduceByKeyAndWindow(_ + _,Seconds(10)).map{case (topic, count) => (count, topic)}.transform(_.sortByKey(false))
 topCounts60.foreachRDD(rdd => {
 val topList = rdd.take(10)
 println(" Popular topics in last 60 seconds (%s total):".format(rdd.count()))
 topList.foreach{case (count, tag) => println("%s (%s tweets)".format(tag, count))}
 })
 topCounts10.foreachRDD(rdd => {val topList = rdd.take(10)
 println("\nPopular topics in last 10 seconds (%s total):".format(rdd.count()))
 topList.foreach{case (count, tag) => println("%s (%s tweets)".format(tag, count))}})
 ssc.start()
 ssc.awaitTermination()
 }

}